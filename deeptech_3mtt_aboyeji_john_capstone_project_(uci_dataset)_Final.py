# -*- coding: utf-8 -*-
"""DeepTech_3MTT_ABOYEJI_JOHN_Capstone_Project_(UCI_Dataset) (2000).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NP2eBTzN8ws9_Psm1EcRaEmtFpbBcVMH

#**INTRODUCTION**

### **Heart Disease Prediction Based on Research Data**
This project aims to develop models that predict whether an individual is at risk of having a heart disease using the different features from a health research dataset. The type of Machine learning task I will engage with  is a classification task.

In this project, I will be performing data inspection, Exploratory Data Analysis, data preprocessing, feature engineering, model development, hyperparameter tuning, and model evaluation.

# **1. Import Libraries**
"""

# Basic Libraries
import numpy as np
import pandas as pd

# Visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Preprocessing
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
!pip install xgboost
import xgboost as xgb
from xgboost import XGBClassifier


# Hyperparameter tuning
from sklearn.model_selection import GridSearchCV


# Evaluation
from sklearn.metrics import (accuracy_score,
                             precision_score,
                             recall_score,
                             classification_report,
                             f1_score,
                             roc_auc_score,
                             confusion_matrix,
                             roc_curve,
                             RocCurveDisplay,
                             auc
                             )
from sklearn.linear_model import Lasso, Ridge
from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, RFE
from sklearn.compose import ColumnTransformer

# Save Model
import joblib

"""#**2.	Data Importation**"""

# Load Dataset
file = "/content/heart_disease_uci.csv"
df = pd.read_csv(file)

"""#**3.	Data Inspection**"""

df

df.shape

"""**Insight:**

The dataset has **920 rows** and **16 columns**
"""

df.info()

"""**Insight:**

There are 8 **numeric data type** and 8 **object data type**

####**(a). Handling missing values**
"""

# Checking if there are missing values

df.isnull().sum()

df.info()

# Replacing null values in numerical Features with median value

cols_num = [
    'trestbps',
'chol',
'thalch',
'oldpeak',
'ca',
]


df[cols_num] = df[cols_num].fillna(df[cols_num].median())

# Replacing null values in categorical Features with modal value

cols_cat = [ 'fbs',
            'restecg',
             'exang',
             'slope',
             'thal' ]


for col in cols_cat:
    df[col].fillna(df[col].mode()[0], inplace=True)

# confirming that there are no missing values

df.isnull().sum()

"""####**(b). Dropping Irrelevant Features**"""

df.head(15)

"""#### On inspecting the first  rows in the dataset, the dataset has Features that are insignificant to the task to be achieved


*   dataset
*   id



"""

# Dropping Irrelevant Features

df.drop(['dataset','id'], axis=1, inplace=True)
df

"""####**(c).** Converting **object, boolean data type** to **categorical data type**"""

df.info()

#Converting these columns to categorical data type

df['sex'] = df['sex'].astype('category')
df['cp'] = df['cp'].astype('category')
df['fbs'] = df['fbs'].astype('category')
df['restecg'] = df['restecg'].astype('category')
df['exang'] = df['exang'].astype('category')
df['slope'] = df['slope'].astype('category')
df['thal'] = df['thal'].astype('category')

"""**Reasons for the conversion:**

i) To save memory (especially for this large datasets)

ii) To provide easier understanding and separation of variable types

iii) To make  analysis faster

iv) To prepare the data for preprocessing
"""

df.info()

"""###**(d) Handling Duplicates**"""

# Checking if there are duplicates

duplicates = df.duplicated().sum()
print(f"\nNumber of duplicate rows: {duplicates}")

"""###**(e). Finding unique items in the dataset**"""

#  separating 'numeric' data types from 'non-numeric' data types in the dataset

df_catgorical = df.select_dtypes(exclude = ['number'])
df_num = df.select_dtypes(include = ['number'])

# Finding the number of unique items in 'non-numeric' data types

df_catgorical.nunique()

# Finding each unique items in the 'df_catgorical' dataset
# using for loop

for col in df_catgorical:
  print('Unique items in',str.upper(col),'Column',
          df[col].unique(),
          print('--' * 50))

"""**Insight:**

There are no insignificant data

# **4. Feature Engineering**
"""

df.info()

"""###**(a). Creating a new column, 'age_range',  from 'age' column via feature binning**

"""

# Creating new labels for the age column
labels = ['10-20','21-30','31-40',
          '41-50','51-60', '61-70',
          '71-80', '81-90', '91-100']

# Creating the bins for the labels
bins = [10,20,30,40,50,60,70,80,90,100]

# Creating the 'age_range' column
df["age_range"] = pd.cut(df["age"], bins = bins, labels =labels)

df["age_range"].unique()

"""###**Reasons for the feature binning:**
  
i) To make computation and analysis better and faster  

ii) To provide easier and better understanding and appreciation of the dataset

iii) To prepare the data for preprocessing

#**5. Exploratory Data Analysis**
"""

# Heart disease status Distribution

# Determimning the size of the bar chart
fig, ax = plt.subplots(figsize=(4, 4))

# Creating the bar chart
sns.countplot(x='num', data=df)
plt.title('heart disease status Distribution')
plt.show()

"""**Insight:**

There are, significantly and approximately same number of people having heart diseases(1,2,3,4) as those not having heart disease .
"""

# Age Distribution

# Determimning the size of the Histogram
fig, ax = plt.subplots(figsize=(7, 4))

# Creating the Histogram
sns.histplot(df['age_range'], bins=30, kde=False)
plt.title('Age Distribution')
plt.show()

"""**Insight:**

i). The age range with the highest number of people is '51 - 60', followed by '41 - 50', then '61 -70'.

ii). The age range with the lowest number of people is '21-30'.

iii). The dataset shows there are higher population of people within the age range of 51 years to 60 years in the region.

"""

# Heart disease status by Age Range

# Determimning the size of the clustered bar chart
fig, ax = plt.subplots(figsize=(9, 4))

# Creating the clustered bar chart
sns.countplot(data=df, x='age_range', hue='num')
plt.xticks(rotation=0)
plt.show()

"""**Insight:**

i). The age range with the **highest** number of people having heart disease is 51-60.

ii). The age range with the **lowest** number of people having heart disease is 21-30.

iii). Generally as age increases from 20 to 60, the number of people having heart disease increases drastically.

iv). Generally as age increases from 70 to 80, the number of people without heart disease maintains a very high frequency. This could be as a result of insufficient sample collection of people within that age range in the population
"""

# heart disease status by sex

# Determimning the size of the clustered bar chart
fig, ax = plt.subplots(figsize=(10, 5))

# Creating the clustered bar chart
sns.countplot(data=df, x='sex', hue='num')
plt.xticks(rotation=45)
plt.show()

"""**Insight:**

i). Male tends to be at risk of heart disease more than female

ii). The data seems to capture more data on Male than Female wich might indicate inequality in access to health care facility

# **6.  Data Preprocessing**

####**(a).	Encoding Categorical Variables**
"""

# Using LabelEncoder() to encode the data for Model development
lab_enc = LabelEncoder()

# List of categorical columns to encode
df_catgorical = df.select_dtypes(include = ['object','category'])

# Apply Label Encoding to each categorical column via for loop
for col in df_catgorical:
    df[col] = lab_enc.fit_transform(df[col])

"""#### Correlation Heatmap"""

# Correlation Heatmap

# Determimning the size of the Heatmap
fig, ax = plt.subplots(figsize=(12, 6))

# Creating the Heatmap
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', ax=ax)
plt.title('Correlation Heatmap')
plt.xticks(rotation=60)
plt.show()

"""####**(b) feature and target separation**"""

# Separating the Features from Target in the dataset

X = df.drop('num', axis=1)
y = df['num']

"""####**(c).	Feature Scaling**"""

# Scaling the features using StandardScaler()

scaler = StandardScaler()
scaled_X = scaler.fit_transform(X)

"""#**7.  Model Development**

####**(a). Train-Test Split**
"""

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2, random_state=42)

df['num'].value_counts()

"""**Insight:**

The dataset is imbalanced

***Solution:***

  apply hyperparameter tuning like **'class weight': ['balance']** to some of the models to address this issue.

####**(b). Model Selection, Training, and evaluation**

###**(i). Logistic regression model**
"""

# Define the model
Log_Reg = LogisticRegression()


# Define the grid of hyperparameters
param_grid = {
    'max_iter' : [10,50,70,100,200,500,1000],
    'penalty' : ['l2'],
    'C' : [0.001,0.01,0.1,1,10,100],
    'random_state': [42],
    'class_weight': ['balanced'], #-To address imbalance in dataset
    'multi_class': ['multinomial'],
    'solver': ['lbfgs']

}




# Set up GridSearchCV
LogReg_grid_search = GridSearchCV(estimator=Log_Reg,
                               param_grid=param_grid,
                               cv=5,
                               n_jobs=-1,
                               scoring='f1_macro')

# Fit the model
LogReg_grid_search.fit(X_train, y_train)

# Print the best parameters and score
print("Best parameters found: ", LogReg_grid_search.best_params_)
print("Best cross-validation F1-macro: ", LogReg_grid_search.best_score_)

# Predictions
Log_Reg_model = LogReg_grid_search.best_estimator_
logReg_y_pred = Log_Reg_model.predict(X_test)
logReg_y_pred_proba = Log_Reg_model.predict_proba(X_test)
logReg_y_pred

# Evaluation Metrics For Logistic regression

print("Accuracy:", accuracy_score(y_test, logReg_y_pred))
print("Precision:", precision_score(y_test, logReg_y_pred, average='macro'))
print("Recall:", recall_score(y_test, logReg_y_pred, average='macro'))
print("F1 Score:", f1_score(y_test, logReg_y_pred, average='macro'))
print("ROC-AUC:", roc_auc_score(y_test, logReg_y_pred_proba, average='macro', multi_class='ovr'))
print(classification_report(y_test, logReg_y_pred))

# Confusion Matrix

# Determimning the size of the plot
fig, ax = plt.subplots(figsize=(4, 2))

# Creating the Confusion Matrix
sns.heatmap(confusion_matrix(y_test, logReg_y_pred), annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.show()

"""











###**(ii) K-Nearest Neighbours Model**"""

# Define the model
knn = KNeighborsClassifier()


# Define the grid of hyperparameters
param_grid = {
    'n_neighbors': [3, 5, 7,9,11,13,15,17,19,21,23,
                    25,27,29,31,33,35,37,39,41,43],
    'weights': ['uniform', 'distance'],
    'p': [1, 2],


}

# Set up GridSearchCV
grid_search = GridSearchCV(estimator=knn,
                           param_grid=param_grid,
                           cv=5,
                           n_jobs=-1,
                           scoring='f1_macro')

# Fit the model
grid_search.fit(X_train, y_train)

# Print the best parameters and score
print("Best parameters found: ", grid_search.best_params_)
print("Best cross-validation F1-macro: ", grid_search.best_score_)

# Prediction

best_model = grid_search.best_estimator_
y_pred_knn = best_model.predict(X_test)
y_pred_proba_knn = best_model.predict_proba(X_test)
y_pred_knn

# # Evaluation Metrics For K-Nearest Neighbours Model

print("Accuracy:", accuracy_score(y_test, y_pred_knn))
print("Precision:", precision_score(y_test, y_pred_knn, average='macro'))
print("Recall:", recall_score(y_test, y_pred_knn, average='macro'))
print("F1 Score:", f1_score(y_test, y_pred_knn, average='macro'))
print("ROC-AUC:", roc_auc_score(y_test, y_pred_proba_knn, average='macro', multi_class='ovr'))
print(confusion_matrix(y_test, y_pred_knn))
print(classification_report(y_test, y_pred_knn))


# Confusion Matrix

# Determimning the size of the plot
fig, ax = plt.subplots(figsize=(4, 2))

# Creating the Confusion Matrix
sns.heatmap(confusion_matrix(y_test, y_pred_knn), annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.show()

"""###**(iii) SVM Model**"""

# Define the model
svm_clasf = svm.SVC(probability=True)


# Define the grid of hyperparameters
param_grid = {
    'C': [0.1, 1,3,5,10],
    'gamma': [0.1, 0.05,0.01],
    'kernel': ['linear', 'rbf','poly'],
    'degree': [1, 2,3,4,8],
    'class_weight': [None, 'balanced'],


}




# Set up GridSearchCV
svm_grid_search = GridSearchCV(estimator=svm_clasf,
                               param_grid=param_grid,
                               cv=5,
                               n_jobs=-1,
                               scoring='f1_macro')

# Fit the model
svm_grid_search.fit(X_train, y_train)

# Print the best parameters and score
print("Best parameters found: ", svm_grid_search.best_params_)
print("Best cross-validation F1-macro: ", svm_grid_search.best_score_)

# Make Predictions:

best_model_params = svm_grid_search.best_params_

# Create a new SVC instance with best parameters and explicitly ensure probability=True
best_model = svm.SVC(probability=True,
                     C=best_model_params['C'],
                     gamma=best_model_params['gamma'],
                     kernel=best_model_params['kernel'],
                     degree=best_model_params['degree'],
                     class_weight=best_model_params['class_weight'])

# Fit this new best_model on the training data
best_model.fit(X_train, y_train)

y_pred_SVM = best_model.predict(X_test)
# Get predicted probabilities for ROC-AUC
y_pred_proba_SVM = best_model.predict_proba(X_test)
y_pred_SVM

# Evaluation Metrics For  SVM Model

print("Accuracy:", accuracy_score(y_test, y_pred_SVM))
print("Precision:", precision_score(y_test, y_pred_SVM, average='macro', zero_division=0))
print("Recall:", recall_score(y_test, y_pred_SVM, average='macro', zero_division=0))
print("F1 Score:", f1_score(y_test, y_pred_SVM, average='macro', zero_division=0))
print("ROC-AUC:", roc_auc_score(y_test, y_pred_proba_SVM, average='macro', multi_class='ovr'))
print(classification_report(y_test, y_pred_SVM, zero_division=0))


# Confusion Matrix

# Determimning the size of the plot
fig, ax = plt.subplots(figsize=(4, 2))

# Creating the Confusion Matrix
sns.heatmap(confusion_matrix(y_test, y_pred_SVM), annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.show()

"""###**(iv) Decision Tree Classifier Model**"""

# Define the model
dt_clasf = DecisionTreeClassifier()


# Define the grid of hyperparameters
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 5, 10, 15, 20],
    'min_samples_split': [2, 5, 10,15,20],
    'min_samples_leaf': [1, 3, 5,7,9,10],
    'class_weight': ['balanced'], #-To address imbalance in dataset
    'max_features': ['sqrt', 'log2', None],


}




# Set up GridSearchCV
dt_clasf_grid_search = GridSearchCV(estimator=dt_clasf,
                                    param_grid=param_grid,
                                    cv=5,
                                    n_jobs=-1,
                                    scoring='f1_macro')

# Fit the model
dt_clasf_grid_search.fit(X_train, y_train)

# Print the best parameters and score
print("Best parameters found: ", dt_clasf_grid_search.best_params_)
print("Best cross-validation F1-macro: ", dt_clasf_grid_search.best_score_)

# Make predictions
best_model = dt_clasf_grid_search.best_estimator_
y_pred_dt_clasf = best_model.predict(X_test)
y_pred_proba_dt_clasf = best_model.predict_proba(X_test)
y_pred_dt_clasf

# Evaluation Metrics For Decision Tree Classifier Model

print("Accuracy:", accuracy_score(y_test, y_pred_dt_clasf))
print("Precision:", precision_score(y_test, y_pred_dt_clasf, average='macro', zero_division=0))
print("Recall:", recall_score(y_test, y_pred_dt_clasf, average='macro', zero_division=0))
print("F1 Score:", f1_score(y_test, y_pred_dt_clasf, average='macro', zero_division=0))
print("ROC-AUC:", roc_auc_score(y_test, y_pred_proba_dt_clasf, average='macro', multi_class='ovr'))
print(classification_report(y_test, y_pred_dt_clasf, zero_division=0))


# Confusion Matrix

# Determimning the size of the plot
fig, ax = plt.subplots(figsize=(4, 2))

# Creating the Confusion Matrix
sns.heatmap(confusion_matrix(y_test, y_pred_dt_clasf), annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.show()

"""###**(v) Random Forest Classifier Model**"""

# Define the model
rfc = RandomForestClassifier()


# Define the grid of hyperparameters
param_grid = {
    'n_estimators': [10,20,50],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 3, 5],
    'criterion': ['gini', 'entropy'],
    'max_features': ['sqrt', 'log2'],
    'bootstrap': [True, False],
    'class_weight': ['balanced'], #-To address imbalance dataset

}


# Set up GridSearchCV
rfc_grid_search = GridSearchCV(estimator=rfc,
                               param_grid=param_grid,
                               cv=5,
                               n_jobs=-1,
                               scoring='f1_macro')

# Fit the model
rfc_grid_search.fit(X_train, y_train)

# Print the best parameters and score
print("Best parameters found: ", rfc_grid_search.best_params_)
print("Best cross-validation F1-macro: ", rfc_grid_search.best_score_)

# Make predictions on the test set
best_model = rfc_grid_search.best_estimator_
y_pred_rfc = best_model.predict(X_test)
y_pred_proba_rfc = best_model.predict_proba(X_test)
y_pred_rfc

# Evaluation Metrics For Decision Tree Classifier Model

print("Accuracy:", accuracy_score(y_test, y_pred_rfc))
print("Precision:", precision_score(y_test, y_pred_rfc, average='macro', zero_division=0))
print("Recall:", recall_score(y_test, y_pred_rfc, average='macro', zero_division=0))
print("F1 Score:", f1_score(y_test, y_pred_rfc, average='macro', zero_division=0))
print("ROC-AUC:", roc_auc_score(y_test, y_pred_proba_rfc, average='macro', multi_class='ovr'))
print(classification_report(y_test, y_pred_rfc, zero_division=0))


# Confusion Matrix

# Determimning the size of the plot
fig, ax = plt.subplots(figsize=(4, 2))

# Creating the Confusion Matrix
sns.heatmap(confusion_matrix(y_test, y_pred_rfc), annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.show()

"""###**(vi) AdaBoost Classifier Model**"""

# Define the model
ABClasf_ =  AdaBoostClassifier(estimator=DecisionTreeClassifier())


# Define the grid of hyperparameters
param_grid = {
    'n_estimators': [10,20,50,100,150,200,300],
    'learning_rate': [0.01, 0.05,0.1, 0.5,1.0],
    'estimator__max_depth': [1, 2, 3, 4],


}


# Set up GridSearchCV
abc_grid_search = GridSearchCV(estimator=ABClasf_,
                               param_grid=param_grid,
                               cv=5,
                               n_jobs=-1,
                               scoring='f1_macro')

# Fit the model
abc_grid_search.fit(X_train, y_train)

# Print the best parameters and score
print("Best parameters found: ", abc_grid_search.best_params_)
print("Best cross-validation F1-macro: ", abc_grid_search.best_score_)

# Make predictions on the test set
best_model = abc_grid_search.best_estimator_
y_pred_abc = best_model.predict(X_test)
y_pred_proba_abc = best_model.predict_proba(X_test)
y_pred_abc

# Evaluation Metrics For AdaBoost Classifier Model

print("Accuracy:", accuracy_score(y_test, y_pred_abc))
print("Precision:", precision_score(y_test, y_pred_abc, average='macro', zero_division=0))
print("Recall:", recall_score(y_test, y_pred_abc, average='macro', zero_division=0))
print("F1 Score:", f1_score(y_test, y_pred_abc, average='macro', zero_division=0))
print("ROC-AUC:", roc_auc_score(y_test, y_pred_proba_abc, average='macro', multi_class='ovr'))
print(classification_report(y_test, y_pred_abc, zero_division=0))


# Confusion Matrix

# Determimning the size of the plot
fig, ax = plt.subplots(figsize=(4, 2))

# Creating the Confusion Matrix
sns.heatmap(confusion_matrix(y_test, y_pred_abc), annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.show()

"""###**(vii) Gradient Boosting Classifier Model**"""

# Define the model
GBClasf =  GradientBoostingClassifier()


# Define the grid of hyperparameters
param_grid = {
    'n_estimators': [10,20,50,100,200,],
    'learning_rate': [0.01,0.05, 0.1, 1.0],
    'max_depth': [2,3,4, 5,],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 3],
    'max_features': ['sqrt', 'log2'],





}


# Set up GridSearchCV
GBC_grid_search = GridSearchCV(estimator=GBClasf,
                               param_grid=param_grid,
                               cv=5,
                               n_jobs=-1,
                               scoring='f1_macro')

# Fit the model
GBC_grid_search.fit(X_train, y_train)

# Print the best parameters and score
print("Best parameters found: ", GBC_grid_search.best_params_)
print("Best cross-validation F1-macro: ", GBC_grid_search.best_score_)

# Make predictions on the test set
best_model = GBC_grid_search.best_estimator_
y_pred_GBC = best_model.predict(X_test)
y_pred_proba_GBC = best_model.predict_proba(X_test)
y_pred_GBC

# Evaluation Metrics For Gradient Boosting Classifier Model

print("Accuracy:", accuracy_score(y_test, y_pred_GBC))
print("Precision:", precision_score(y_test, y_pred_GBC, average='macro', zero_division=0))
print("Recall:", recall_score(y_test, y_pred_GBC, average='macro', zero_division=0))
print("F1 Score:", f1_score(y_test, y_pred_GBC, average='macro', zero_division=0))
print("ROC-AUC:", roc_auc_score(y_test, y_pred_proba_GBC, average='macro', multi_class='ovr'))
print(classification_report(y_test, y_pred_GBC, zero_division=0))


# Confusion Matrix

# Determimning the size of the plot
fig, ax = plt.subplots(figsize=(4, 2))

# Creating the Confusion Matrix
sns.heatmap(confusion_matrix(y_test, y_pred_GBC), annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.show()

"""###**(viii) XGBoost classifier Model**"""

# Define the model

xgb_model = xgb.XGBClassifier()


# Define the grid of hyperparameters
param_grid = {

    'n_estimators': [50, 100, 150],
    'learning_rate': [0.01, 0.05, 0.2],
    'max_depth': [3, 5, 6],
    'subsample': [0.5, 0.7, 0.9],
    'scale_pos_weight': [1, 2, 3],
    'gamma': [0, 0.1, 0.2],
    'min_child_weight': [1, 3, 5],
    'colsample_bytree': [0.7, 0.8, 1.0],



}


# Set up GridSearchCV
XGB_grid_search = GridSearchCV(estimator=xgb_model,
                               param_grid=param_grid,
                               cv=5,
                               n_jobs=-1,
                               scoring='f1_macro')


# Fit the model
XGB_grid_search.fit(X_train, y_train)

# Print the best parameters and score
print("Best parameters found: ", XGB_grid_search.best_params_)
print("Best cross-validation F1-macro: ", XGB_grid_search.best_score_)

# Make predictions on the test set
best_model = XGB_grid_search.best_estimator_
y_pred_XGB = best_model.predict(X_test)
y_pred_proba_XGB = best_model.predict_proba(X_test)
y_pred_XGB

# Evaluation Metrics For XGBoost classifier Model

print("Accuracy:", accuracy_score(y_test, y_pred_XGB))
print("Precision:", precision_score(y_test, y_pred_XGB, average='weighted', zero_division=0))
print("Recall:", recall_score(y_test, y_pred_XGB, average='weighted', zero_division=0))
print("F1 Score:", f1_score(y_test, y_pred_XGB, average='weighted', zero_division=0))
print("ROC-AUC:", roc_auc_score(y_test, y_pred_proba_XGB, average='weighted', multi_class='ovr'))
print(classification_report(y_test, y_pred_XGB, zero_division=0))


# Confusion Matrix

# Determimning the size of the plot
fig, ax = plt.subplots(figsize=(4, 2))

# Creating the Confusion Matrix
sns.heatmap(confusion_matrix(y_test, y_pred_XGB), annot=True, fmt='d')
plt.title('Confusion Matrix')
plt.show()

"""##**8. Ploting ROC_AUC score and curve for all the Models**"""

from sklearn.preprocessing import label_binarize

# Binarize y_test for macro-average ROC
classes = np.unique(y_test)
y_test_binarized = label_binarize(y_test, classes=classes)

# --- Calculate ROC curve and AUC for each model using macro-average ---

# Logistic Regression
fpr_Log, tpr_Log, _ = roc_curve(y_test_binarized.ravel(), logReg_y_pred_proba.ravel())
auc_Log = auc(fpr_Log, tpr_Log)

# K-Nearest Neighbours Model
fpr_KNN, tpr_KNN, _ = roc_curve(y_test_binarized.ravel(), y_pred_proba_knn.ravel())
auc_KNN = auc(fpr_KNN, tpr_KNN)

# SVM Model
fpr_SVM, tpr_SVM, _ = roc_curve(y_test_binarized.ravel(), y_pred_proba_SVM.ravel())
auc_SVM = auc(fpr_SVM, tpr_SVM)

# Decision Tree Classifier Model
fpr_DT_clasf, tpr_DT_clasf, _ = roc_curve(y_test_binarized.ravel(), y_pred_proba_dt_clasf.ravel())
auc_DT_clasf = auc(fpr_DT_clasf, tpr_DT_clasf)

# Random Forest Classifier Model
fpr_RFC, tpr_RFC, _ = roc_curve(y_test_binarized.ravel(), y_pred_proba_rfc.ravel())
auc_RFC = auc(fpr_RFC, tpr_RFC)

# AdaBoost Classifier Model
fpr_ABC, tpr_ABC, _ = roc_curve(y_test_binarized.ravel(), y_pred_proba_abc.ravel())
auc_ABC = auc(fpr_ABC, tpr_ABC)

# Gradient Boosting Classifier Model
fpr_GBC, tpr_GBC, _ = roc_curve(y_test_binarized.ravel(), y_pred_proba_GBC.ravel())
auc_GBC = auc(fpr_GBC, tpr_GBC)

# XGBoost classifier Model
fpr_XGB, tpr_XGB, _ = roc_curve(y_test_binarized.ravel(), y_pred_proba_XGB.ravel())
auc_XGB = auc(fpr_XGB, tpr_XGB)


# --- Plot the ROC_AUC curve ---
plt.figure(figsize=(10, 7)) # Adjust figure size for better readability
plt.plot(fpr_Log, tpr_Log, label=f"Logistic regression AUC SCORE: {auc_Log:.2f}")
plt.plot(fpr_KNN, tpr_KNN,  label=f"K-Nearest Neighbours Model AUC SCORE: {auc_KNN:.2f}")
plt.plot(fpr_SVM, tpr_SVM,  label=f"SVM Model AUC SCORE: {auc_SVM:.2f}")
plt.plot(fpr_DT_clasf, tpr_DT_clasf, label=f"Decision Tree Classifier Model AUC SCORE: {auc_DT_clasf:.2f}")
plt.plot(fpr_RFC, tpr_RFC,  label=f"Random Forest Classifier Model AUC SCORE: {auc_RFC:.2f}")
plt.plot(fpr_ABC, tpr_ABC,  label=f"AdaBoost Classifier Model: {auc_ABC:.2f}")
plt.plot(fpr_GBC, tpr_GBC, label=f"Gradient Boosting Classifier Model AUC SCORE: {auc_GBC:.2f}")
plt.plot(fpr_XGB, tpr_XGB,  label=f"XGBoost classifier Model AUC SCORE: {auc_XGB:.2f}")

plt.plot([0, 1], [0, 1], 'k--') # Random guess line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.ylabel("True Positive Rate")
plt.xlabel("False Positive Rate")
plt.title("Macro-Averaged ROC Curves for All Models")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

"""- The best models  (XGBoost Classifier  with tuned hyperparameters) had the highest ROC_AUC score, and achieved a good balance between precision and recall,  indicating better performance than other models"""

# Saving the models

A_best_model = y_pred_rfc
joblib.dump(A_best_model, 'rfc_heart_disease_predictor_model.joblib')


B_best_model = y_pred_knn
joblib.dump(B_best_model, 'KNN_heart_disease_predictor_model.joblib')

C_best_model = y_pred_SVM
joblib.dump(C_best_model, 'SVM_heart_disease_predictor_model.joblib')

D_best_model = y_pred_XGB
joblib.dump(D_best_model, 'XGB_heart_disease_predictor_model.joblib')

"""## Summary:
**Data inspection and cleaning**
- Irrelevant column was dropped
- Rows with missing data and duplicates were removed
- Irrelevant data  was removed
- Relevant columns needing conversion were converted to categorical data type.

**Feature engineering**
- Feature binning was done on some features  for easier exploratory data analysis and better data preprocessing
-New features such as  'age_range' were created from existing features for better appreciation of the data.

**Exploratory Data Analysis**

- Male tends to be at risk of heart disease more than female
- The data seems to capture more data on Male than Female wich might indicate inequality in access to health care facility
- The age range with the **highest** number of people having heart disease is 51-60.

**Data preprocessing**
- Data was encoded for model training
- Features were scaled to ensure that all  features contribute equally to the model's learning process.
- PCA reduced feature space while preserving 95% variance, improving speed without much performance loss.

**Model training and evaluation**
- The best models  (Random Forest Classifier Model, XGBoost Classifier, K-Nearest Neeighbours, and SVM Model,  with tuned hyperparameters) had the highest ROC_AUC score, and achieved a good balance between precision and recall,  indicating better performance than other models

## Limitations:
- Higher number of estimators could not be applied in hyperparameter tuning due to insufficient and unavailable computational resources for model training
- Number of hyperparameters applied were reduced as well due to inefficient computational resources for model training
- A relatively small dataset which may affect learning process of the models

## Recommendations(for better acurracy and performance of the trained models):
- Access to efficient and proficient computational resources to have higher accurate-performing models ( training and evaluation).
- Collecting more granular and relevant data features like job role(sedentary life-style), industry sector( exposure to hazards), or location.
- Exploring deep learning for further improvement and development of more models.
- Availability of Data from Nigeria and Other African Health research centres and Hospitals to prevent Bias in model training
"""

